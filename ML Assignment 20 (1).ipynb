{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4fa1a7-e53d-4861-8c53-e7ac02f5caca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. What is the underlying concept of Support Vector Machines?\n",
    "\n",
    "Ans. The underlying concept of Support Vector Machines (SVMs) is to find an optimal hyperplane that separates different classes of data \n",
    "points in a high-dimensional space. SVMs are a type of supervised machine learning algorithm used for classification and regression tasks.\n",
    "\n",
    "The key idea behind SVMs is to map the input data points into a higher-dimensional feature space where it becomes easier to classify them \n",
    "linearly. In this feature space, SVMs aim to find a hyperplane that maximally separates the data points of different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dd5ee6-3879-41dd-b02c-afe5f3b3cec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. What is the concept of a support vector?\n",
    "\n",
    "Ans. After training an SVM, a support vector is any instance located on the “street” (see the previous answer), including its border. The \n",
    "decision boundary is entirely determined by the support vectors. Any instance that is not a support vector (i.e., off the street) has no \n",
    "influence whatsoever; you could remove them, add more instances, or move them around, and as long as they stay off the street they won’t \n",
    "affect the decision boundary. Computing the predictions only involves the support vectors, not the whole training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df25fa8-28e5-4d0a-83e3-057ef4ce9cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. When using SVMs, why is it necessary to scale the inputs?\n",
    "\n",
    "Ans. SVMs try to fit the largest possible “street” between the classes (see the first answer), so if the training set is not scaled, the \n",
    "SVM will tend to neglect small features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8461123f-0c56-4f92-9a2c-55c5379f9461",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. When an SVM classifier classifies a case, can it output a confidence score? What about a percentage chance?\n",
    "\n",
    "Ans. An SVM classifier can output the distance between the test instance and the decision boundary, and you can use this as a confidence \n",
    "score. However, this score cannot be directly converted into an estimation of the class probability. If you set probability=True when \n",
    "creating an SVM in Scikit-Learn, then after training it will calibrate the probabilities using Logistic Regression on the SVM’s \n",
    "scores (trained by an additional five-fold cross-validation on the training data). This will add the predict_proba() and predict_log_proba() \n",
    "methods to the SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50caa22b-f635-4769-8a65-13d6b1e4b723",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Should you train a model on a training set with millions of instances and hundreds of features using the primal or dual form of the SVM problem?\n",
    "\n",
    "Ans. This question applies only to linear SVMs since kernelized can only use the dual form. The computational complexity of the primal form \n",
    "of the SVM problem is proportional to the number of training instances m, while the computational complexity of the dual form is \n",
    "proportional to a number between m2 and m3. So if there are millions of instances, you should definitely use the primal form, because the\n",
    "dual form will be much too slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f517f4e-c02d-4d37-a08c-24c8b75b23e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. Lets say you've used an RBF kernel to train an SVM classifier, but it appears to underfit the training collection. Is it better to raise or lower (gamma)? What about the letter C?\n",
    "\n",
    "Ans. If an SVM classifier trained with an RBF kernel underfits the training set, there might be too much regularization. To decrease it,\n",
    "you need to increase gamma or C (or both)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9892d5d-29d8-4091-aaed-c7167f6079c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. To solve the soft margin linear SVM classifier problem with an off-the-shelf QP solver, how should the QP parameters (H, f, A, and b) be set?\n",
    "\n",
    "Ans. Let’s call the QP parameters for the hard-margin problem H′, f′, A′ and b′. The QP parameters for the soft-margin problem have m \n",
    "additional parameters (np = n + 1 + m) and m additional constraints (nc = 2m). They can be defined like so:\n",
    "\n",
    "*H is equal to H′, plus m columns of 0s on the right and m rows of 0s at the bottom\n",
    "*f is equal to f′ with m additional elements, all equal to the value of the hyperparameter C.\n",
    "*b is equal to b′ with m additional elements, all equal to 0.\n",
    "*A is equal to A′, with an extra m × m identity matrix Im appended to the right,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
